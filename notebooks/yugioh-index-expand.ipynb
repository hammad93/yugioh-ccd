{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9086948,"sourceType":"datasetVersion","datasetId":5477627}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Yugioh TCG Complete Card Database\nThis notebook will perform we scraping to load all available cards and their sets from scratch. This is divided into 2 main processing tasks and an additional merge from 3rd party sources for the collectibles market. The first 2 tasks are done in this notebook.\n\n  1. **Indexing**: This will index every unique card as it relates to the trading card game which official rules will treat identically\n  2. **Expanding**: Each unique card has multiple rarities, releases, and sets and this task will find them all.\n  3. **Merging**: Once we have the data from official sources, we can merge data from 3rd parties like TCGPlayer to understand things like the price.","metadata":{}},{"cell_type":"markdown","source":"## Rules\nThe code is meant to not only be compliant but respectful to the web servers that have the data. The scraper will not only follow robots.txt but also optimize further. In the Appendix, there's code that provides an aggressive way that is literally 1000x faster but will overwhelm the data sources.","metadata":{}},{"cell_type":"code","source":"import requests\nprint(requests.get('https://tcgplayer.com/robots.txt').text)","metadata":{"id":"NsNhEhb-90mp","outputId":"18080298-0ea8-453f-8663-1b48ae2e342a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 404 file not found response, no safeguards\nrequests.get('https://www.yugioh-card.com/robots.txt')","metadata":{"id":"oc8IPIqA971O","outputId":"26c84d2b-292b-4cbf-febb-3b244f0bc0cd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Index Configuration & Example\nConfiguration details including the URL's that need to be predefined are done here. A basic example is provided for understanding and to make sure it will work in the environment. Then, we loop through all cards based on the Example.","metadata":{}},{"cell_type":"code","source":"# this is the card search from the front end where we are going to start loading the cards\nconfig = {\n    'base-url' : 'https://www.db.yugioh-card.com/yugiohdb/card_search.action?ope=1&sess=1&rp=10&mode=&sort=1&keyword=&stype=1&ctype=&othercon=2&starfr=&starto=&pscalefr=&pscaleto=&linkmarkerfr=&linkmarkerto=&link_m=2&atkfr=&atkto=&deffr=&defto=&releaseDStart=1&releaseMStart=1&releaseYStart=1999&releaseDEnd=&releaseMEnd=&releaseYEnd=&page=',\n    'index-url' : 'https://www.db.yugioh-card.com/yugiohdb/card_search.action?ope=2&cid=',\n    'link-monster-img': 'https://www.db.yugioh-card.com/yugiohdb/external/image/parts/link_pc/link{P}.png'\n}","metadata":{"id":"BlP17v4E-FS9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup","metadata":{"id":"eRO-XJVi_ROv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = requests.get(config['base-url'])","metadata":{"id":"5snRnK6n_S53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"soup = BeautifulSoup(example.text, 'html.parser')","metadata":{"id":"R81457xa_Vcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"soup","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hand = soup.find_all('div', class_ = 't_row')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scrape Index\nThe Yugioh card index from Konami is paginated with 10 cards in each page. We will go through and parse all cards on each page. The following functions will process the page, or `hand`, and add them to results","metadata":{}},{"cell_type":"code","source":"def transform_hand(hand):\n    '''\n    Takes in the HTML row and returns an appropriate dictionary\n\n    Description: Description of the card's effects or abilities.\n    Name: The card's name.\n    Attribute: The attribute of the monster (e.g., LIGHT, DARK).\n    Level/Rank: item_box_valueLevel for normal monsters or rank for Xyz monsters.\n    Attack: Attack points of the monster.\n    Defense: Defense points of the monster.\n    Type: Card type (e.g., Monster, Spell, Trap).\n    SubType: Specific type within the card type (e.g., Dragon, Warrior).\n    '''\n    results = []\n    for card in hand:\n        soup = BeautifulSoup(str(card), 'html.parser')\n        # these are fields shared by all cards\n        \n        result = {\n            'index': soup.find('input', {'class': 'cid'})['value'],\n            'name': soup.find('span', {'class': 'card_name'}).text,\n            'description': soup.find('dd', {'class': 'box_card_text'}).text.strip(),\n            'type': soup.find('span', {'class': 'box_card_attribute'}).text.strip()\n        }\n\n        # based on the type, each card can have different fields\n        if result['type'] in ['SPELL', 'TRAP'] :\n            sub_type = soup.find('span', {'class': 'box_card_effect'})\n            result.update({\n                'sub_type': sub_type.text.strip() if sub_type else '',\n                'attribute': '',\n                'rank': '',\n                'attack': '',\n                'defense': ''\n            })\n        else : # MONSTER card\n            # check if link card\n            link = soup.find('span', {'class': 'box_card_linkmarker'})\n            if link :\n                rank = soup.find('span', {'class': 'box_card_linkmarker'}).text.strip()\n                links = soup.find('img', {'title': 'Link'})['src'].split('/')[-1].replace('.png', '').replace('link', '')\n                rank = f'{rank} P{links}'\n            else:\n                rank = soup.find('span', {'class': 'box_card_level_rank'}).text.strip()\n            result.update({\n                'type': 'MONSTER',\n                'attribute': result['type'],\n                'sub_type': soup.find('span', {'class': 'card_info_species_and_other_item'}).text.replace('\\r\\n', '').replace('\\t', ''),\n                'rank': rank,\n                'attack': soup.find('span', {'class': 'atk_power'}).text.strip().split()[-1],\n                'defense': soup.find('span', {'class': 'def_power'}).text.strip().split()[-1]\n            })\n        \n        results.append(result)\n    \n    return results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cards = transform_hand(hand)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"page = 2 # we're on page 2 after example\nhand = cards","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate through database and collect links to cards\nwhile len(hand) > 0:\n  print(page)\n  data = None\n  \n  while data is None:\n    try: # connect\n      data = requests.get(config['base-url'] + str(page))\n    except:\n      print('trying again')\n      pass\n  \n  soup = BeautifulSoup(data.text, 'html.parser')\n  hand = transform_hand(soup.find_all('div', class_ = 't_row'))\n  cards.extend(hand)\n  page += 1","metadata":{"id":"ipQQWWpFIuMc","outputId":"46936bd3-69d4-4650-afc7-ed960ff44a9d","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cards)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deck = cards","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Index\nWe will save out the index to a CSV file. Please make sure to rename it.","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(deck)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('202408141845.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Expand Configuration and Example\nEach card has a variety of information that can be parsed from the detailed page. This includes release dates and sets the part was part of.","metadata":{}},{"cell_type":"code","source":"example = requests.get(config['index-url'] + df.iloc[456]['index'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deck = list(df['index'])\nprint(len(deck))\nprint(deck[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('example.html', 'w+') as f :\n    f.write(example.text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Expand Scrape\nBased on the index parameters, we can now query the official Konami database to request all of the details available about the card. Because there is so much data, we will save out all details and then process it. The data is the same from the card website and each HTML file will be saved out into a folder called `output`.","metadata":{}},{"cell_type":"code","source":"import os\n\ncompleted = os.listdir('output/')\nfor i, index in enumerate(deck) :\n    print(f'\\rCard {i} ID: {index}{\" \"*50}', end='')\n    \n    filename = f'output/{index}.html'\n    \n    if f'{index}.html' in completed :\n        print(f'Found {filename} already, skipping.')\n        continue\n\n    response = None\n    while (not response) :\n        try:\n            response = requests.get(config['index-url'] + index)\n        except:\n            print(f'Retrying')\n            response = None\n\n    if response.ok :\n        with open(filename, 'w+') as f:\n            f.write(response.text)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"card_files = os.listdir('output/')\nlen(card_files)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define relevant columns\nindex_df = df\ndf = df[['index', 'name', 'description', 'type',\n       'sub_type', 'attribute', 'rank', 'attack', 'defense']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\ncards = []\nfor i, index in enumerate(df['index']) :\n    # extract card from database\n    card_default = df[df['index'] == index].to_dict(orient='records')[0]\n\n    # extract card details\n    file_path = f'output/{index}.html'\n    with open(file_path, 'r') as f:\n        data = f.read()\n        soup = BeautifulSoup(data, 'html.parser')\n        sets = soup.find('div', {'id': 'update_list'}).find_all('div', class_='t_row')\n    # loaded all sets, now parse\n    if len(sets) < 1 :\n        print('error , no sets found: ' + card['name'])\n    for set in sets :\n        card = card_default.copy()\n        # gets set name, release, card set id\n        set_soup = BeautifulSoup(str(set), 'html.parser')\n        card.update({\n            'set_name': set_soup.find('div', {'class': 'pack_name'}).text.strip(),\n            'set_id': set_soup.find('div', {'class': 'card_number'}).text.strip(),\n            'set_release': set_soup.find('div', {'class': 'time'}).text.strip(),\n            'rarity': ' '.join(set_soup.find('div', {'class': 'lr_icon'}).text.split())\n        })\n        cards.append(card)\n        print(f\"\\rGenerated {card['set_id']} {card['rarity']} {i+1} {len(df)} {' '*50}\", end='')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cards)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finalize Complete Card Database\nWe will now save out our expanded version but make sure to change the filename. It is considered complete in terms of official data sources. Additional data like market prices are merged but this is considered 3rd party.","metadata":{}},{"cell_type":"code","source":"full_df = pd.DataFrame(cards)\nfull_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.to_csv('202408142150.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Appendix","metadata":{}},{"cell_type":"code","source":"'''\n# reference https://www.scrapingbee.com/tutorials/make-concurrent-requests-in-python/\nimport concurrent.futures\nimport requests\n\nMAX_RETRIES = 5 # Setting the maximum number of retries if we have failed requests to 5.\nMAX_THREADS = 1000\n\ndef scrape(url):\n    for _ in range(MAX_RETRIES):\n        response = requests.get(config['index-url'] + url) # Scrape!\n\n        if response.ok: # If we get a successful request\n            print(index)\n            index = url.split('=')[-1] #the cid parameter in the url\n            with open(f'output/{index}.html', 'w+') as f:\n                f.write(response.text)\n            \n        else: # If we get a failed request, then we continue the loop\n            print(response.content)\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n    executor.map(scrape, list(set(deck)))\n'''","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\noutputs = os.listdir('output/')\nprint(len(outputs))","metadata":{},"execution_count":null,"outputs":[]}]}